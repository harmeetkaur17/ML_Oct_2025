{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc84df1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHAT IS CLASSIFICATION?\n",
      "=========================\n",
      "\n",
      "ğŸ¯ CLASSIFICATION = Predicting categories/classes for new data\n",
      "\n",
      "ğŸ“Š Key Characteristics:\n",
      "   â€¢ Output: Discrete categories (labels)\n",
      "   â€¢ Goal: Assign input to one of predefined classes\n",
      "   â€¢ Examples: Spam/Not Spam, Cat/Dog, Medical Diagnosis\n",
      "   \n",
      "ğŸ”„ Types of Classification:\n",
      "   â€¢ Binary Classification: 2 classes (Yes/No, True/False)\n",
      "   â€¢ Multi-class: 3+ classes (Red/Green/Blue)\n",
      "   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"WHAT IS CLASSIFICATION?\")\n",
    "print(\"=\"*25)\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ¯ CLASSIFICATION = Predicting categories/classes for new data\n",
    "\n",
    "ğŸ“Š Key Characteristics:\n",
    "   â€¢ Output: Discrete categories (labels)\n",
    "   â€¢ Goal: Assign input to one of predefined classes\n",
    "   â€¢ Examples: Spam/Not Spam, Cat/Dog, Medical Diagnosis\n",
    "   \n",
    "ğŸ”„ Types of Classification:\n",
    "   â€¢ Binary Classification: 2 classes (Yes/No, True/False)\n",
    "   â€¢ Multi-class: 3+ classes (Red/Green/Blue)\n",
    "   \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03f42a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ REAL-WORLD CLASSIFICATION EXAMPLES\n",
      "=============================================\n",
      "\n",
      "1. Email Spam Detection\n",
      "   Input: Email content, sender, subject\n",
      "   Output: Spam or Not Spam\n",
      "   Type: Binary Classification\n",
      "\n",
      "2. Image Recognition\n",
      "   Input: Pixel values of image\n",
      "   Output: Cat, Dog, Bird, Car, etc.\n",
      "   Type: Multi-class Classification\n",
      "\n",
      "3. Medical Diagnosis\n",
      "   Input: Patient symptoms, test results\n",
      "   Output: Disease A, Disease B, Healthy\n",
      "   Type: Multi-class Classification\n",
      "\n",
      "4. Customer Segmentation\n",
      "   Input: Age, income, purchase history\n",
      "   Output: Premium, Standard, Budget\n",
      "   Type: Multi-class Classification\n",
      "\n",
      "5. Sentiment Analysis\n",
      "   Input: Text review or comment\n",
      "   Output: Positive, Negative, Neutral\n",
      "   Type: Multi-class Classification\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ“ REAL-WORLD CLASSIFICATION EXAMPLES\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "classification_examples = [\n",
    "    {\n",
    "        \"Problem\": \"Email Spam Detection\",\n",
    "        \"Input\": \"Email content, sender, subject\",\n",
    "        \"Output\": \"Spam or Not Spam\",\n",
    "        \"Type\": \"Binary Classification\"\n",
    "    },\n",
    "    {\n",
    "        \"Problem\": \"Image Recognition\", \n",
    "        \"Input\": \"Pixel values of image\",\n",
    "        \"Output\": \"Cat, Dog, Bird, Car, etc.\",\n",
    "        \"Type\": \"Multi-class Classification\"\n",
    "    },\n",
    "    {\n",
    "        \"Problem\": \"Medical Diagnosis\",\n",
    "        \"Input\": \"Patient symptoms, test results\",\n",
    "        \"Output\": \"Disease A, Disease B, Healthy\",\n",
    "        \"Type\": \"Multi-class Classification\"\n",
    "    },\n",
    "    {\n",
    "        \"Problem\": \"Customer Segmentation\",\n",
    "        \"Input\": \"Age, income, purchase history\",\n",
    "        \"Output\": \"Premium, Standard, Budget\",\n",
    "        \"Type\": \"Multi-class Classification\"\n",
    "    },\n",
    "    {\n",
    "        \"Problem\": \"Sentiment Analysis\",\n",
    "        \"Input\": \"Text review or comment\",\n",
    "        \"Output\": \"Positive, Negative, Neutral\",\n",
    "        \"Type\": \"Multi-class Classification\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, example in enumerate(classification_examples, 1):\n",
    "    print(f\"\\n{i}. {example['Problem']}\")\n",
    "    print(f\"   Input: {example['Input']}\")\n",
    "    print(f\"   Output: {example['Output']}\")\n",
    "    print(f\"   Type: {example['Type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c8c2b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ğŸ”§ GENERAL CLASSIFICATION WORKFLOW\n",
      "==========================================\n",
      "\n",
      "PSEUDO CODE: Classification Pipeline\n",
      "=====================================\n",
      "\n",
      "1. DATA PREPARATION\n",
      "   â””â”€â”€ Load dataset\n",
      "   â””â”€â”€ Handle missing values\n",
      "   â””â”€â”€ Encode categorical variables\n",
      "   â””â”€â”€ Split features (X) and target (y)\n",
      "\n",
      "2. DATA SPLITTING\n",
      "   â””â”€â”€ Split data into train/test sets\n",
      "   â””â”€â”€ Optional: Create validation set\n",
      "\n",
      "3. MODEL TRAINING\n",
      "   â””â”€â”€ Choose classification algorithm\n",
      "   â””â”€â”€ Train model on training data\n",
      "   â””â”€â”€ Model learns patterns: X â†’ y\n",
      "\n",
      "4. MODEL EVALUATION\n",
      "   â””â”€â”€ Make predictions on test set\n",
      "   â””â”€â”€ Calculate accuracy, precision, recall\n",
      "   â””â”€â”€ Analyze confusion matrix\n",
      "\n",
      "5. PREDICTION\n",
      "   â””â”€â”€ Use trained model on new data\n",
      "   â””â”€â”€ Get predicted class labels\n",
      "\n",
      "\n",
      "PSEUDO CODE TEMPLATE:\n",
      "=========================\n",
      "\n",
      "// Step 1: Data Preparation\n",
      "LOAD dataset\n",
      "X = features (input variables)\n",
      "y = target (class labels)\n",
      "\n",
      "// Step 2: Split Data\n",
      "X_train, X_test, y_train, y_test = SPLIT(X, y, test_size=0.2)\n",
      "\n",
      "// Step 3: Train Model\n",
      "model = CHOOSE_ALGORITHM()  // e.g., Decision Tree, SVM, etc.\n",
      "model.FIT(X_train, y_train)\n",
      "\n",
      "// Step 4: Make Predictions\n",
      "y_predicted = model.PREDICT(X_test)\n",
      "\n",
      "// Step 5: Evaluate Performance\n",
      "accuracy = CALCULATE_ACCURACY(y_test, y_predicted)\n",
      "DISPLAY results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nğŸ”§ GENERAL CLASSIFICATION WORKFLOW\")\n",
    "print(\"=\"*42)\n",
    "\n",
    "print(\"\"\"\n",
    "PSEUDO CODE: Classification Pipeline\n",
    "=====================================\n",
    "\n",
    "1. DATA PREPARATION\n",
    "   â””â”€â”€ Load dataset\n",
    "   â””â”€â”€ Handle missing values\n",
    "   â””â”€â”€ Encode categorical variables\n",
    "   â””â”€â”€ Split features (X) and target (y)\n",
    "\n",
    "2. DATA SPLITTING\n",
    "   â””â”€â”€ Split data into train/test sets\n",
    "   â””â”€â”€ Optional: Create validation set\n",
    "\n",
    "3. MODEL TRAINING\n",
    "   â””â”€â”€ Choose classification algorithm\n",
    "   â””â”€â”€ Train model on training data\n",
    "   â””â”€â”€ Model learns patterns: X â†’ y\n",
    "\n",
    "4. MODEL EVALUATION\n",
    "   â””â”€â”€ Make predictions on test set\n",
    "   â””â”€â”€ Calculate accuracy, precision, recall\n",
    "   â””â”€â”€ Analyze confusion matrix\n",
    "\n",
    "5. PREDICTION\n",
    "   â””â”€â”€ Use trained model on new data\n",
    "   â””â”€â”€ Get predicted class labels\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nPSEUDO CODE TEMPLATE:\")\n",
    "print(\"=\"*25)\n",
    "\n",
    "print(\"\"\"\n",
    "// Step 1: Data Preparation\n",
    "LOAD dataset\n",
    "X = features (input variables)\n",
    "y = target (class labels)\n",
    "\n",
    "// Step 2: Split Data\n",
    "X_train, X_test, y_train, y_test = SPLIT(X, y, test_size=0.2)\n",
    "\n",
    "// Step 3: Train Model\n",
    "model = CHOOSE_ALGORITHM()  // e.g., Decision Tree, SVM, etc.\n",
    "model.FIT(X_train, y_train)\n",
    "\n",
    "// Step 4: Make Predictions\n",
    "y_predicted = model.PREDICT(X_test)\n",
    "\n",
    "// Step 5: Evaluate Performance\n",
    "accuracy = CALCULATE_ACCURACY(y_test, y_predicted)\n",
    "DISPLAY results\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66ed1784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ğŸŒŸ EXAMPLE 1: EMAIL SPAM CLASSIFICATION\n",
      "=============================================\n",
      "\n",
      "PROBLEM: Classify emails as Spam or Not Spam\n",
      "INPUT: Email features (word counts, sender info, etc.)\n",
      "OUTPUT: Spam (1) or Not Spam (0)\n",
      "\n",
      "SAMPLE DATA:\n",
      "Email | Word_Count | Has_Links | From_Unknown | Spam\n",
      "------|------------|-----------|--------------|------\n",
      "  1   |     50     |     0     |      0       |  0\n",
      "  2   |    200     |     5     |      1       |  1  \n",
      "  3   |     30     |     1     |      0       |  0\n",
      "  4   |    500     |    10     |      1       |  1\n",
      "\n",
      "\n",
      "PSEUDO CODE: Spam Classification\n",
      "===================================\n",
      "\n",
      "// Step 1: Load and Prepare Data\n",
      "LOAD email_dataset\n",
      "X = [word_count, has_links, from_unknown]  // Features\n",
      "y = [spam_label]                           // Target (0=Not Spam, 1=Spam)\n",
      "\n",
      "// Step 2: Split Data\n",
      "X_train, X_test, y_train, y_test = SPLIT(X, y, test_size=0.3)\n",
      "\n",
      "// Step 3: Choose and Train Model\n",
      "model = Algorithm()\n",
      "model.FIT(X_train, y_train)\n",
      "\n",
      "// Step 4: Make Predictions\n",
      "FOR each email in X_test:\n",
      "    prediction = model.PREDICT(email)\n",
      "    IF prediction == 1:\n",
      "        PRINT \"This email is SPAM\"\n",
      "    ELSE:\n",
      "        PRINT \"This email is NOT SPAM\"\n",
      "\n",
      "// Step 5: Evaluate Model\n",
      "accuracy = CALCULATE_ACCURACY(y_test, predictions)\n",
      "PRINT \"Model Accuracy: \" + accuracy + \"%\"\n",
      "\n",
      "// Step 6: Use for New Emails\n",
      "new_email = [75, 2, 0]  // word_count=75, has_links=2, from_unknown=0\n",
      "result = model.PREDICT(new_email)\n",
      "IF result == 1:\n",
      "    PRINT \"SPAM DETECTED!\"\n",
      "ELSE:\n",
      "    PRINT \"Email is safe\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nğŸŒŸ EXAMPLE 1: EMAIL SPAM CLASSIFICATION\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "print(\"\"\"\n",
    "PROBLEM: Classify emails as Spam or Not Spam\n",
    "INPUT: Email features (word counts, sender info, etc.)\n",
    "OUTPUT: Spam (1) or Not Spam (0)\n",
    "\n",
    "SAMPLE DATA:\n",
    "Email | Word_Count | Has_Links | From_Unknown | Spam\n",
    "------|------------|-----------|--------------|------\n",
    "  1   |     50     |     0     |      0       |  0\n",
    "  2   |    200     |     5     |      1       |  1  \n",
    "  3   |     30     |     1     |      0       |  0\n",
    "  4   |    500     |    10     |      1       |  1\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nPSEUDO CODE: Spam Classification\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "spam_pseudo_code = \"\"\"\n",
    "// Step 1: Load and Prepare Data\n",
    "LOAD email_dataset\n",
    "X = [word_count, has_links, from_unknown]  // Features\n",
    "y = [spam_label]                           // Target (0=Not Spam, 1=Spam)\n",
    "\n",
    "// Step 2: Split Data\n",
    "X_train, X_test, y_train, y_test = SPLIT(X, y, test_size=0.3)\n",
    "\n",
    "// Step 3: Choose and Train Model\n",
    "model = Algorithm()\n",
    "model.FIT(X_train, y_train)\n",
    "\n",
    "// Step 4: Make Predictions\n",
    "FOR each email in X_test:\n",
    "    prediction = model.PREDICT(email)\n",
    "    IF prediction == 1:\n",
    "        PRINT \"This email is SPAM\"\n",
    "    ELSE:\n",
    "        PRINT \"This email is NOT SPAM\"\n",
    "\n",
    "// Step 5: Evaluate Model\n",
    "accuracy = CALCULATE_ACCURACY(y_test, predictions)\n",
    "PRINT \"Model Accuracy: \" + accuracy + \"%\"\n",
    "\n",
    "// Step 6: Use for New Emails\n",
    "new_email = [75, 2, 0]  // word_count=75, has_links=2, from_unknown=0\n",
    "result = model.PREDICT(new_email)\n",
    "IF result == 1:\n",
    "    PRINT \"SPAM DETECTED!\"\n",
    "ELSE:\n",
    "    PRINT \"Email is safe\"\n",
    "\"\"\"\n",
    "\n",
    "print(spam_pseudo_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af12fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHAT IS LOGISTIC REGRESSION?\n",
      "================================\n",
      "\n",
      "ğŸ¯ LOGISTIC REGRESSION = Classification algorithm using probability\n",
      "\n",
      "ğŸ“Š Key Concepts:\n",
      "   â€¢ Uses logistic/sigmoid function to map any value to 0-1 range\n",
      "   â€¢ Outputs probability of belonging to a class\n",
      "   â€¢ Decision boundary: if probability > 0.5 â†’ Class 1, else Class 0\n",
      "   \n",
      "   \n",
      "ğŸ§® Mathematical Foundation:\n",
      "   \n",
      "   â€¢ Logistic Regression: p = 1 / (1 + e^(-(mx + b))) (outputs 0-1)\n",
      "   \n",
      "ğŸ“ˆ The Sigmoid Function:\n",
      "   â€¢ S-shaped curve that maps (-âˆ, +âˆ) to (0, 1)\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"WHAT IS LOGISTIC REGRESSION?\")\n",
    "print(\"=\"*32)\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ¯ LOGISTIC REGRESSION = Classification algorithm using probability\n",
    "\n",
    "ğŸ“Š Key Concepts:\n",
    "   â€¢ Uses logistic/sigmoid function to map any value to 0-1 range\n",
    "   â€¢ Outputs probability of belonging to a class\n",
    "   â€¢ Decision boundary: if probability > 0.5 â†’ Class 1, else Class 0\n",
    "   \n",
    "   \n",
    "ğŸ§® Mathematical Foundation:\n",
    "   \n",
    "   â€¢ Logistic Regression: p = 1 / (1 + e^(-(mx + b))) (outputs 0-1)\n",
    "   \n",
    "ğŸ“ˆ The Sigmoid Function:\n",
    "   â€¢ S-shaped curve that maps (-âˆ, +âˆ) to (0, 1)\n",
    "\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a995a20",
   "metadata": {},
   "source": [
    "A confusion matrix is a table used to evaluate the performance of a classification model, comparing its predictions to the actual outcomes. It breaks down correct and incorrect predictions into four key values: true positives, true negatives, false positives, and false negatives. This provides a more detailed understanding than accuracy alone, especially with unbalanced datasets. \n",
    "Understanding the four components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8468caac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "2. UNDERSTANDING THE FOUR COMPONENTS\n",
      "=============================================\n",
      "ğŸ“§ SPAM EMAIL EXAMPLE:\n",
      "=========================\n",
      "\n",
      "Let's say we have 100 emails and our spam detector makes predictions:\n",
      "\n",
      "ACTUAL SITUATION:\n",
      "â€¢ 60 emails are actually HAM (not spam)\n",
      "â€¢ 40 emails are actually SPAM\n",
      "\n",
      "MODEL PREDICTIONS:\n",
      "â€¢ Model predicts 55 as HAM\n",
      "â€¢ Model predicts 45 as SPAM\n",
      "\n",
      "CONFUSION MATRIX BREAKDOWN:\n",
      "\n",
      "                 PREDICTED\n",
      "              Ham    Spam\n",
      "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "ACTUAL Ham â”‚  50     10  â”‚\n",
      "      Spam â”‚   5     35  â”‚\n",
      "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ” DETAILED BREAKDOWN:\n",
      "   TN (True Negative) = 50: Ham emails correctly identified as Ham âœ…\n",
      "   FP (False Positive) = 10: Ham emails wrongly labeled as Spam âŒ\n",
      "   FN (False Negative) = 5: Spam emails missed (labeled as Ham) âŒ\n",
      "   TP (True Positive) = 35: Spam emails correctly identified as Spam âœ…\n",
      "\n",
      "ğŸ“Š WHAT THIS TELLS US:\n",
      "   â€¢ Model correctly identified 85 out of 100 emails\n",
      "   â€¢ Model made 15 errors\n",
      "   â€¢ 10 innocent emails marked as spam (annoying for users)\n",
      "   â€¢ 5 spam emails got through (security risk)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n2. UNDERSTANDING THE FOUR COMPONENTS\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# Create a visual example\n",
    "print(\"ğŸ“§ SPAM EMAIL EXAMPLE:\")\n",
    "print(\"=\"*25)\n",
    "\n",
    "print(\"\"\"\n",
    "Let's say we have 100 emails and our spam detector makes predictions:\n",
    "\n",
    "ACTUAL SITUATION:\n",
    "â€¢ 60 emails are actually HAM (not spam)\n",
    "â€¢ 40 emails are actually SPAM\n",
    "\n",
    "MODEL PREDICTIONS:\n",
    "â€¢ Model predicts 55 as HAM\n",
    "â€¢ Model predicts 45 as SPAM\n",
    "\n",
    "CONFUSION MATRIX BREAKDOWN:\n",
    "\"\"\")\n",
    "\n",
    "# Example confusion matrix\n",
    "actual_labels = ['Ham', 'Spam']\n",
    "predicted_labels = ['Ham', 'Spam']\n",
    "\n",
    "# Example data: [TN, FP], [FN, TP]\n",
    "example_cm = np.array([[50, 10],  # 50 correctly identified ham, 10 ham misclassified as spam\n",
    "                       [5, 35]])   # 5 spam missed, 35 correctly identified spam\n",
    "\n",
    "print(\"                 PREDICTED\")\n",
    "print(\"              Ham    Spam\")\n",
    "print(\"         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(f\"ACTUAL Ham â”‚  {example_cm[0,0]:2d}     {example_cm[0,1]:2d}  â”‚\")\n",
    "print(f\"      Spam â”‚  {example_cm[1,0]:2d}     {example_cm[1,1]:2d}  â”‚\")\n",
    "print(\"         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "\n",
    "# Explain each component\n",
    "tn, fp, fn, tp = example_cm[0,0], example_cm[0,1], example_cm[1,0], example_cm[1,1]\n",
    "\n",
    "print(f\"\\nğŸ” DETAILED BREAKDOWN:\")\n",
    "print(f\"   TN (True Negative) = {tn}: Ham emails correctly identified as Ham âœ…\")\n",
    "print(f\"   FP (False Positive) = {fp}: Ham emails wrongly labeled as Spam âŒ\")\n",
    "print(f\"   FN (False Negative) = {fn}: Spam emails missed (labeled as Ham) âŒ\")\n",
    "print(f\"   TP (True Positive) = {tp}: Spam emails correctly identified as Spam âœ…\")\n",
    "\n",
    "print(f\"\\nğŸ“Š WHAT THIS TELLS US:\")\n",
    "print(f\"   â€¢ Model correctly identified {tn + tp} out of 100 emails\")\n",
    "print(f\"   â€¢ Model made {fp + fn} errors\")\n",
    "print(f\"   â€¢ {fp} innocent emails marked as spam (annoying for users)\")\n",
    "print(f\"   â€¢ {fn} spam emails got through (security risk)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7639667",
   "metadata": {},
   "source": [
    "An ACCURACY SCORE is a metric that measures the proportion of correct predictions a model makes out of all predictions. It is calculated by dividing the number of correct predictions by the total number of predictions. The score ranges from \\(0.0\\) to \\(1.0\\) (or \\(0\\%\\) to \\(100\\%\\)), with \\(1.0\\) representing a perfect score where all predictions are correct.Â "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0f27655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. WHAT IS ACCURACY SCORE?\n",
      "==============================\n",
      "\n",
      "ğŸ¯ ACCURACY = Proportion of correct predictions out of all predictions\n",
      "\n",
      "ğŸ“Š Definition:\n",
      "   â€¢ Measures overall correctness of a classification model\n",
      "   â€¢ Simple and intuitive metric\n",
      "   â€¢ Most commonly used evaluation metric\n",
      "   \n",
      "ğŸ“ Formula:\n",
      "   Accuracy = (Correct Predictions) / (Total Predictions)\n",
      "   Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
      "   \n",
      "ğŸ“ˆ Range:\n",
      "   â€¢ Minimum: 0.0 (0%) - All predictions wrong\n",
      "   â€¢ Maximum: 1.0 (100%) - All predictions correct\n",
      "   â€¢ Higher values = Better performance\n",
      "\n",
      "ğŸ”¢ SIMPLE EXAMPLE:\n",
      "====================\n",
      "Actual:    [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
      "Predicted: [1, 0, 1, 0, 0, 1, 0, 1, 1, 0]\n",
      "Matches:   [1, 1, 1, 0, 1, 1, 1, 0, 1, 1]\n",
      "\n",
      "Calculation:\n",
      "Correct predictions: 8\n",
      "Total predictions: 10\n",
      "Accuracy: 8/10 = 0.80 = 80.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"1. WHAT IS ACCURACY SCORE?\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ¯ ACCURACY = Proportion of correct predictions out of all predictions\n",
    "\n",
    "ğŸ“Š Definition:\n",
    "   â€¢ Measures overall correctness of a classification model\n",
    "   â€¢ Simple and intuitive metric\n",
    "   â€¢ Most commonly used evaluation metric\n",
    "   \n",
    "ğŸ“ Formula:\n",
    "   Accuracy = (Correct Predictions) / (Total Predictions)\n",
    "   Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "   \n",
    "ğŸ“ˆ Range:\n",
    "   â€¢ Minimum: 0.0 (0%) - All predictions wrong\n",
    "   â€¢ Maximum: 1.0 (100%) - All predictions correct\n",
    "   â€¢ Higher values = Better performance\n",
    "\"\"\")\n",
    "\n",
    "print(\"ğŸ”¢ SIMPLE EXAMPLE:\")\n",
    "print(\"=\"*20)\n",
    "\n",
    "# Simple example\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
    "y_pred = [1, 0, 1, 0, 0, 1, 0, 1, 1, 0]\n",
    "\n",
    "correct = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)\n",
    "total = len(y_true)\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f\"Actual:    {y_true}\")\n",
    "print(f\"Predicted: {y_pred}\")\n",
    "print(f\"Matches:   {[1 if t==p else 0 for t, p in zip(y_true, y_pred)]}\")\n",
    "print(f\"\\nCalculation:\")\n",
    "print(f\"Correct predictions: {correct}\")\n",
    "print(f\"Total predictions: {total}\")\n",
    "print(f\"Accuracy: {correct}/{total} = {accuracy:.2f} = {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e56b18",
   "metadata": {},
   "source": [
    "Precision is a machine learning metric that measures how many of a model's positive predictions were correct, helping to minimize false positives. It is calculated as the number of true positives divided by the total number of positive predictions (true positives + false positives). A high precision means that when the model predicts a positive outcome, it is very likely to be correct. \n",
    "\n",
    "Minimizing false positives: Precision is especially important when the cost of a false positive is high. For example, in a medical diagnosis model, high precision is crucial to ensure that a diagnosis of \"sick\" is rarely wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cae0a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. WHAT IS PRECISION?\n",
      "=========================\n",
      "\n",
      "ğŸ¯ PRECISION = How many positive predictions were actually correct\n",
      "\n",
      "ğŸ“Š Definition:\n",
      "   â€¢ Measures accuracy of positive predictions\n",
      "   â€¢ Focuses on minimizing false positives\n",
      "   â€¢ Answers: \"Of all positive predictions, how many were right?\"\n",
      "   \n",
      "ğŸ“ Formula:\n",
      "   Precision = True Positives / (True Positives + False Positives)\n",
      "   Precision = TP / (TP + FP)\n",
      "   \n",
      "ğŸ“ˆ Range:\n",
      "   â€¢ Minimum: 0.0 (0%) - All positive predictions wrong\n",
      "   â€¢ Maximum: 1.0 (100%) - All positive predictions correct\n",
      "   â€¢ Higher values = Better performance\n",
      "   \n",
      "ğŸ¯ Key Question:\n",
      "   \"When the model says YES, how often is it actually YES?\"\n",
      "\n",
      "ğŸ” PRECISION vs ACCURACY:\n",
      "   ğŸ“Š Accuracy: Overall correctness (all predictions)\n",
      "   ğŸ¯ Precision: Correctness of positive predictions only\n",
      "   ğŸ“ˆ Accuracy: Considers all four confusion matrix values\n",
      "   ğŸ” Precision: Only considers TP and FP\n",
      "   âš–ï¸ Accuracy: Good for balanced datasets\n",
      "   ğŸ¯ Precision: Critical when false positives are costly\n"
     ]
    }
   ],
   "source": [
    "print(\"1. WHAT IS PRECISION?\")\n",
    "print(\"=\"*25)\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ¯ PRECISION = How many positive predictions were actually correct\n",
    "\n",
    "ğŸ“Š Definition:\n",
    "   â€¢ Measures accuracy of positive predictions\n",
    "   â€¢ Focuses on minimizing false positives\n",
    "   â€¢ Answers: \"Of all positive predictions, how many were right?\"\n",
    "   \n",
    "ğŸ“ Formula:\n",
    "   Precision = True Positives / (True Positives + False Positives)\n",
    "   Precision = TP / (TP + FP)\n",
    "   \n",
    "ğŸ“ˆ Range:\n",
    "   â€¢ Minimum: 0.0 (0%) - All positive predictions wrong\n",
    "   â€¢ Maximum: 1.0 (100%) - All positive predictions correct\n",
    "   â€¢ Higher values = Better performance\n",
    "   \n",
    "ğŸ¯ Key Question:\n",
    "   \"When the model says YES, how often is it actually YES?\"\n",
    "\"\"\")\n",
    "\n",
    "print(\"ğŸ” PRECISION vs ACCURACY:\")\n",
    "differences = [\n",
    "    \"ğŸ“Š Accuracy: Overall correctness (all predictions)\",\n",
    "    \"ğŸ¯ Precision: Correctness of positive predictions only\", \n",
    "    \"ğŸ“ˆ Accuracy: Considers all four confusion matrix values\",\n",
    "    \"ğŸ” Precision: Only considers TP and FP\",\n",
    "    \"âš–ï¸ Accuracy: Good for balanced datasets\",\n",
    "    \"ğŸ¯ Precision: Critical when false positives are costly\"\n",
    "]\n",
    "\n",
    "for diff in differences:\n",
    "    print(f\"   {diff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f988bb",
   "metadata": {},
   "source": [
    "Medical Diagnosis (e.g., Cancer Screening): A false positive in a serious disease screening (like cancer) can cause immense psychological distress to the patient and lead to unnecessary, expensive, and potentially invasive follow-up procedures and treatments. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e22c05e",
   "metadata": {},
   "source": [
    "Recall is a machine learning metric that measures the ability of a model to find all the relevant cases in a dataset. It is calculated as the number of true positives divided by the total number of actual positives (true positives plus false negatives). A high recall score means the model is good at identifying most of the positive instances, answering the question: \"Of all the actual positives, how many did the model correctly detect?\". \n",
    "\n",
    "Healthcare: In disease prediction, a high recall is crucial to ensure that most actual cases are identified, even if it means some false positives occur. A false negative (missing a disease) can have serious consequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "161972c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. WHAT IS RECALL?\n",
      "====================\n",
      "\n",
      "ğŸ” RECALL = Ability of a model to find all the relevant cases\n",
      "\n",
      "ğŸ“Š Definition:\n",
      "   â€¢ Measures how well we capture all positive instances\n",
      "   â€¢ Focuses on minimizing false negatives\n",
      "   â€¢ Answers: \"Of all actual positives, how many did we find?\"\n",
      "   \n",
      "ğŸ“ Formula:\n",
      "   Recall = True Positives / (True Positives + False Negatives)\n",
      "   Recall = TP / (TP + FN)\n",
      "   \n",
      "ğŸ“ˆ Range:\n",
      "   â€¢ Minimum: 0.0 (0%) - Found no positive cases\n",
      "   â€¢ Maximum: 1.0 (100%) - Found all positive cases\n",
      "   â€¢ Higher values = Better performance\n",
      "   \n",
      "ğŸ¯ Key Question:\n",
      "   \"Of all the actual positives, how many did we correctly detect?\"\n",
      "   \n",
      "ğŸ”„ Other Names:\n",
      "   â€¢ Sensitivity\n",
      "   â€¢ True Positive Rate (TPR)\n",
      "   â€¢ Hit Rate\n",
      "\n",
      "ğŸ” RECALL vs PRECISION:\n",
      "   ğŸ” Recall: Of actual positives, how many did we catch?\n",
      "   ğŸ¯ Precision: Of predicted positives, how many were correct?\n",
      "   ğŸ“ˆ Recall: Focuses on minimizing false negatives (missed cases)\n",
      "   ğŸ¯ Precision: Focuses on minimizing false positives (false alarms)\n",
      "   âš–ï¸ Recall: Critical when missing positives is costly\n",
      "   âš–ï¸ Precision: Critical when false alarms are costly\n"
     ]
    }
   ],
   "source": [
    "print(\"1. WHAT IS RECALL?\")\n",
    "print(\"=\"*20)\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ” RECALL = Ability of a model to find all the relevant cases\n",
    "\n",
    "ğŸ“Š Definition:\n",
    "   â€¢ Measures how well we capture all positive instances\n",
    "   â€¢ Focuses on minimizing false negatives\n",
    "   â€¢ Answers: \"Of all actual positives, how many did we find?\"\n",
    "   \n",
    "ğŸ“ Formula:\n",
    "   Recall = True Positives / (True Positives + False Negatives)\n",
    "   Recall = TP / (TP + FN)\n",
    "   \n",
    "ğŸ“ˆ Range:\n",
    "   â€¢ Minimum: 0.0 (0%) - Found no positive cases\n",
    "   â€¢ Maximum: 1.0 (100%) - Found all positive cases\n",
    "   â€¢ Higher values = Better performance\n",
    "   \n",
    "ğŸ¯ Key Question:\n",
    "   \"Of all the actual positives, how many did we correctly detect?\"\n",
    "   \n",
    "ğŸ”„ Other Names:\n",
    "   â€¢ Sensitivity\n",
    "   â€¢ True Positive Rate (TPR)\n",
    "   â€¢ Hit Rate\n",
    "\"\"\")\n",
    "\n",
    "print(\"ğŸ” RECALL vs PRECISION:\")\n",
    "differences = [\n",
    "    \"ğŸ” Recall: Of actual positives, how many did we catch?\",\n",
    "    \"ğŸ¯ Precision: Of predicted positives, how many were correct?\", \n",
    "    \"ğŸ“ˆ Recall: Focuses on minimizing false negatives (missed cases)\",\n",
    "    \"ğŸ¯ Precision: Focuses on minimizing false positives (false alarms)\",\n",
    "    \"âš–ï¸ Recall: Critical when missing positives is costly\",\n",
    "    \"âš–ï¸ Precision: Critical when false alarms are costly\"\n",
    "]\n",
    "\n",
    "for diff in differences:\n",
    "    print(f\"   {diff}\")\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
