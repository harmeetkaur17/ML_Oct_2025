{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e9643bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCALING IN MACHINE LEARNING\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"SCALING IN MACHINE LEARNING\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3c793eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data (Unscaled):\n",
      "   age  salary  experience\n",
      "0   25   30000           2\n",
      "1   30   50000           5\n",
      "2   35   75000           8\n",
      "3   40   90000          12\n",
      "4   45  120000          15\n",
      "\n",
      "Ranges:\n",
      "Age: 25 - 45\n",
      "Salary: 30000 - 120000\n",
      "Experience: 2 - 15\n",
      "\n",
      "PROBLEM: Salary dominates due to large scale!\n",
      "Machine learning algorithms will focus on salary and ignore age/experience\n"
     ]
    }
   ],
   "source": [
    "# Example: Why scaling matters\n",
    "data_unscaled = pd.DataFrame({\n",
    "    'age': [25, 30, 35, 40, 45],\n",
    "    'salary': [30000, 50000, 75000, 90000, 120000],\n",
    "    'experience': [2, 5, 8, 12, 15]\n",
    "})\n",
    "\n",
    "print(\"Original Data (Unscaled):\")\n",
    "print(data_unscaled)\n",
    "print(f\"\\nRanges:\")\n",
    "print(f\"Age: {data_unscaled['age'].min()} - {data_unscaled['age'].max()}\")\n",
    "print(f\"Salary: {data_unscaled['salary'].min()} - {data_unscaled['salary'].max()}\")\n",
    "print(f\"Experience: {data_unscaled['experience'].min()} - {data_unscaled['experience'].max()}\")\n",
    "\n",
    "print(\"\\nPROBLEM: Salary dominates due to large scale!\")\n",
    "print(\"Machine learning algorithms will focus on salary and ignore age/experience\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10712cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STANDARD SCALER RESULTS:\n",
      "==================================================\n",
      "Scaled Data:\n",
      "        age    salary  experience\n",
      "0 -1.414214 -1.376396   -1.369474\n",
      "1 -0.707107 -0.736212   -0.727533\n",
      "2  0.000000  0.064018   -0.085592\n",
      "3  0.707107  0.544157    0.770329\n",
      "4  1.414214  1.504433    1.412270\n",
      "\n",
      "After scaling - all features have:\n",
      "Mean ‚âà 0, Standard Deviation ‚âà 1\n",
      "Age mean: 0.000000\n",
      "Salary mean: 0.000000\n",
      "Experience mean: -0.000000\n"
     ]
    }
   ],
   "source": [
    "# Standard Scaler - most common scaling method\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply Standard Scaling\n",
    "data_scaled = data_unscaled.copy()\n",
    "data_scaled[['age', 'salary', 'experience']] = scaler.fit_transform(data_unscaled[['age', 'salary', 'experience']])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STANDARD SCALER RESULTS:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Scaled Data:\")\n",
    "print(data_scaled)\n",
    "\n",
    "print(f\"\\nAfter scaling - all features have:\")\n",
    "print(f\"Mean ‚âà 0, Standard Deviation ‚âà 1\")\n",
    "print(f\"Age mean: {data_scaled['age'].mean():.6f}\")\n",
    "print(f\"Salary mean: {data_scaled['salary'].mean():.6f}\")\n",
    "print(f\"Experience mean: {data_scaled['experience'].mean():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9a8e14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HOW STANDARD SCALER WORKS:\n",
      "===================================\n",
      "Formula: z = (x - mean) / standard_deviation\n",
      "\n",
      "Step by step for 'age' column:\n",
      "Original ages: [25, 30, 35, 40, 45]\n",
      "Mean: 35.0\n",
      "Std: 7.905694150420948\n",
      "\n",
      "Transformation:\n",
      "(25 - 35.0) / 7.905694150420948 = -1.265\n",
      "(30 - 35.0) / 7.905694150420948 = -0.632\n",
      "(35 - 35.0) / 7.905694150420948 = 0.000\n",
      "(40 - 35.0) / 7.905694150420948 = 0.632\n",
      "(45 - 35.0) / 7.905694150420948 = 1.265\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nHOW STANDARD SCALER WORKS:\")\n",
    "print(\"=\"*35)\n",
    "print(\"Formula: z = (x - mean) / standard_deviation\")\n",
    "print(\"\\nStep by step for 'age' column:\")\n",
    "\n",
    "age_mean = data_unscaled['age'].mean()\n",
    "age_std = data_unscaled['age'].std()\n",
    "\n",
    "print(f\"Original ages: {data_unscaled['age'].tolist()}\")\n",
    "print(f\"Mean: {age_mean}\")\n",
    "print(f\"Std: {age_std}\")\n",
    "\n",
    "print(\"\\nTransformation:\")\n",
    "for i, age in enumerate(data_unscaled['age']):\n",
    "    scaled_age = (age - age_mean) / age_std\n",
    "    print(f\"({age} - {age_mean}) / {age_std} = {scaled_age:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62c64d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SCALING METHODS GUIDE:\n",
      "================================================================================\n",
      "\n",
      "StandardScaler:\n",
      "  Formula: (x - mean) / std\n",
      "  Output: Mean=0, Std=1\n",
      "  Best For: Normal distribution, most ML algorithms\n",
      "  Algorithms: SVM, Neural Networks, KNN, PCA\n",
      "\n",
      "MinMaxScaler:\n",
      "  Formula: (x - min) / (max - min)\n",
      "  Output: [0, 1]\n",
      "  Best For: Bounded range needed (0-1)\n",
      "  Algorithms: Neural Networks, Image processing\n",
      "\n",
      "RobustScaler:\n",
      "  Formula: (x - median) / IQR\n",
      "  Output: Median=0, robust to outliers\n",
      "  Best For: Data with outliers\n",
      "  Algorithms: When outliers present\n"
     ]
    }
   ],
   "source": [
    "scaling_guide = pd.DataFrame({\n",
    "    'Scaler': ['StandardScaler', 'MinMaxScaler', 'RobustScaler'],\n",
    "    'Formula': [\n",
    "        '(x - mean) / std',\n",
    "        '(x - min) / (max - min)', \n",
    "        '(x - median) / IQR'\n",
    "    ],\n",
    "    'Output Range': [\n",
    "        'Mean=0, Std=1',\n",
    "        '[0, 1]',\n",
    "        'Median=0, robust to outliers'\n",
    "    ],\n",
    "    'Best For': [\n",
    "        'Normal distribution, most ML algorithms',\n",
    "        'Bounded range needed (0-1)',\n",
    "        'Data with outliers'\n",
    "    ],\n",
    "    'Algorithms': [\n",
    "        'SVM, Neural Networks, KNN, PCA',\n",
    "        'Neural Networks, Image processing',\n",
    "        'When outliers present'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SCALING METHODS GUIDE:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx, row in scaling_guide.iterrows():\n",
    "    print(f\"\\n{row['Scaler']}:\")\n",
    "    print(f\"  Formula: {row['Formula']}\")\n",
    "    print(f\"  Output: {row['Output Range']}\")\n",
    "    print(f\"  Best For: {row['Best For']}\")\n",
    "    print(f\"  Algorithms: {row['Algorithms']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9861a3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "REAL WORLD EXAMPLE:\n",
      "==================================================\n",
      "House Data (Unscaled):\n",
      "   house_price  bedrooms  square_feet  lot_size\n",
      "0       200000         2         1200      0.25\n",
      "1       350000         3         1800      0.50\n",
      "2       500000         4         2500      0.75\n",
      "3       750000         4         3200      1.00\n",
      "4      1200000         5         4500      2.00\n",
      "\n",
      "House Data (Standard Scaled):\n",
      "   house_price  bedrooms  square_feet  lot_size\n",
      "0       -1.141    -1.569       -1.256    -1.076\n",
      "1       -0.713    -0.588       -0.733    -0.662\n",
      "2       -0.285     0.392       -0.122    -0.248\n",
      "3        0.428     0.392        0.488     0.166\n",
      "4        1.711     1.373        1.622     1.821\n",
      "\n",
      "Now all features contribute equally to ML algorithms!\n"
     ]
    }
   ],
   "source": [
    "# Real-world example with different scales\n",
    "real_data = pd.DataFrame({\n",
    "    'house_price': [200000, 350000, 500000, 750000, 1200000],\n",
    "    'bedrooms': [2, 3, 4, 4, 5],\n",
    "    'square_feet': [1200, 1800, 2500, 3200, 4500],\n",
    "    'lot_size': [0.25, 0.5, 0.75, 1.0, 2.0]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"REAL WORLD EXAMPLE:\")\n",
    "print(\"=\"*50)\n",
    "print(\"House Data (Unscaled):\")\n",
    "print(real_data)\n",
    "\n",
    "# Apply Standard Scaling\n",
    "scaler_real = StandardScaler()\n",
    "real_scaled = scaler_real.fit_transform(real_data)\n",
    "real_scaled_df = pd.DataFrame(real_scaled, columns=real_data.columns)\n",
    "\n",
    "print(\"\\nHouse Data (Standard Scaled):\")\n",
    "print(real_scaled_df.round(3))\n",
    "\n",
    "print(\"\\nNow all features contribute equally to ML algorithms!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9fa5bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DECISION FLOWCHART:\n",
      "==============================\n",
      "\n",
      "üìä YOUR DATA\n",
      "    ‚îÇ\n",
      "    ‚îú‚îÄ Has OUTLIERS? \n",
      "    ‚îÇ   ‚îÇ\n",
      "    ‚îÇ   ‚îú‚îÄ YES ‚Üí Use RobustScaler\n",
      "    ‚îÇ   ‚îÇ         (Uses median & IQR, ignores outliers)\n",
      "    ‚îÇ   ‚îÇ\n",
      "    ‚îÇ   ‚îî‚îÄ NO ‚Üí Continue...\n",
      "    ‚îÇ\n",
      "    ‚îú‚îÄ Need values in [0,1] range?\n",
      "    ‚îÇ   ‚îÇ\n",
      "    ‚îÇ   ‚îú‚îÄ YES ‚Üí Use MinMaxScaler\n",
      "    ‚îÇ   ‚îÇ         (Neural networks, image data)\n",
      "    ‚îÇ   ‚îÇ\n",
      "    ‚îÇ   ‚îî‚îÄ NO ‚Üí Use StandardScaler\n",
      "    ‚îÇ             (Most common choice)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a decision flowchart\n",
    "print(\"\\nDECISION FLOWCHART:\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "decision_tree = \"\"\"\n",
    "üìä YOUR DATA\n",
    "    ‚îÇ\n",
    "    ‚îú‚îÄ Has OUTLIERS? \n",
    "    ‚îÇ   ‚îÇ\n",
    "    ‚îÇ   ‚îú‚îÄ YES ‚Üí Use RobustScaler\n",
    "    ‚îÇ   ‚îÇ         (Uses median & IQR, ignores outliers)\n",
    "    ‚îÇ   ‚îÇ\n",
    "    ‚îÇ   ‚îî‚îÄ NO ‚Üí Continue...\n",
    "    ‚îÇ\n",
    "    ‚îú‚îÄ Need values in [0,1] range?\n",
    "    ‚îÇ   ‚îÇ\n",
    "    ‚îÇ   ‚îú‚îÄ YES ‚Üí Use MinMaxScaler\n",
    "    ‚îÇ   ‚îÇ         (Neural networks, image data)\n",
    "    ‚îÇ   ‚îÇ\n",
    "    ‚îÇ   ‚îî‚îÄ NO ‚Üí Use StandardScaler\n",
    "    ‚îÇ             (Most common choice)\n",
    "\"\"\"\n",
    "\n",
    "print(decision_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a3e6f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. ROBUST SCALER - Use When:\n",
      "========================================\n",
      "‚úÖ Data has outliers\n",
      "‚úÖ Skewed distributions\n",
      "‚úÖ When median is more representative than mean\n",
      "‚úÖ Financial data, real estate prices\n",
      "‚úÖ When StandardScaler gives poor results\n",
      "\n",
      "Data with outlier:\n",
      "    income  age\n",
      "0    30000   25\n",
      "1    35000   28\n",
      "2    40000   30\n",
      "3    45000   32\n",
      "4    50000   35\n",
      "5  2000000   40\n",
      "\n",
      "Standard Scaler range: -1.37 to 2.24\n",
      "Robust Scaler range: -1.04 to 156.60\n",
      "‚Üí RobustScaler handles outlier better!\n"
     ]
    }
   ],
   "source": [
    "# When to use RobustScaler\n",
    "print(\"\\n2. ROBUST SCALER - Use When:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "robust_examples = [\n",
    "    \"‚úÖ Data has outliers\",\n",
    "    \"‚úÖ Skewed distributions\", \n",
    "    \"‚úÖ When median is more representative than mean\",\n",
    "    \"‚úÖ Financial data, real estate prices\",\n",
    "    \"‚úÖ When StandardScaler gives poor results\"\n",
    "]\n",
    "\n",
    "for example in robust_examples:\n",
    "    print(example)\n",
    "\n",
    "# Example with outliers\n",
    "outlier_demo = pd.DataFrame({\n",
    "    'income': [30000, 35000, 40000, 45000, 50000, 2000000],  # One millionaire!\n",
    "    'age': [25, 28, 30, 32, 35, 40]\n",
    "})\n",
    "\n",
    "print(\"\\nData with outlier:\")\n",
    "print(outlier_demo)\n",
    "\n",
    "# Compare StandardScaler vs RobustScaler\n",
    "scaler_robust = RobustScaler()\n",
    "scaled_robust = scaler_robust.fit_transform(outlier_demo)\n",
    "\n",
    "scaler_standard = StandardScaler() \n",
    "scaled_standard = scaler_standard.fit_transform(outlier_demo)\n",
    "\n",
    "print(f\"\\nStandard Scaler range: {scaled_standard.min():.2f} to {scaled_standard.max():.2f}\")\n",
    "print(f\"Robust Scaler range: {scaled_robust.min():.2f} to {scaled_robust.max():.2f}\")\n",
    "print(\"‚Üí RobustScaler handles outlier better!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176585ba",
   "metadata": {},
   "source": [
    "*************Dimentionality Reduction*****************************\n",
    "\n",
    "\n",
    "Dimensionality Reduction is a technique used to reduce the number of features (dimensions) in a dataset while preserving the most important information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0999b7f1",
   "metadata": {},
   "source": [
    "WHY DIMENSIONALITY REDUCTION?\n",
    "===================================\n",
    "üêå COMPUTATIONAL COST: More features = slower processing\n",
    "üíæ STORAGE: More memory needed to store data\n",
    "üìä VISUALIZATION: Hard to plot >3 dimensions\n",
    "üéØ CURSE OF DIMENSIONALITY: Data becomes sparse in high dimensions\n",
    "üîç OVERFITTING: More features can lead to overfitting\n",
    "üßπ NOISE: Some features may be irrelevant or noisy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e195fbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIMENSIONALITY REDUCTION: CORRELATION & VARIANCE THRESHOLD\n",
      "=================================================================\n",
      "Three simple but powerful feature selection methods:\n",
      "1. Variance Threshold - Remove low variance features\n",
      "2. Correlation Threshold - Remove highly correlated features\n",
      "2. Exhaustive feature Selection - tries all combinations of features\n"
     ]
    }
   ],
   "source": [
    "# Add this cell after your dimensionality reduction markdown\n",
    "\n",
    "print(\"DIMENSIONALITY REDUCTION: CORRELATION & VARIANCE THRESHOLD\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Three simple but powerful feature selection methods:\")\n",
    "print(\"1. Variance Threshold - Remove low variance features\")\n",
    "print(\"2. Correlation Threshold - Remove highly correlated features\")\n",
    "print(\"2. Exhaustive feature Selection - tries all combinations of features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "319ac195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRELATION-BASED DIMENSIONALITY REDUCTION EXAMPLES\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Add this cell to demonstrate correlation-based dimensionality reduction\n",
    "\n",
    "print(\"CORRELATION-BASED DIMENSIONALITY REDUCTION EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770680a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. PHYSICAL MEASUREMENTS EXAMPLE:\n",
      "========================================\n",
      "Physical measurements dataset:\n",
      "   height_cm  height_inches  height_feet  weight_kg  weight_pounds   bmi\n",
      "0        170           66.9         5.57         70            154  24.2\n",
      "1        175           68.9         5.74         80            176  26.1\n",
      "2        165           65.0         5.41         60            132  22.0\n",
      "3        180           70.9         5.91         90            198  27.8\n",
      "4        160           63.0         5.25         55            121  21.5\n",
      "5        185           72.8         6.07         95            209  27.7\n",
      "6        172           67.7         5.64         75            165  25.3\n",
      "7        168           66.1         5.51         65            143  23.0\n",
      "8        177           69.7         5.81         85            187  27.1\n",
      "9        162           63.8         5.31         58            128  22.1\n",
      "\n",
      "Correlation Matrix:\n",
      "               height_cm  height_inches  height_feet  weight_kg  \\\n",
      "height_cm          1.000          1.000        1.000      0.993   \n",
      "height_inches      1.000          1.000        1.000      0.993   \n",
      "height_feet        1.000          1.000        1.000      0.993   \n",
      "weight_kg          0.993          0.993        0.993      1.000   \n",
      "weight_pounds      0.992          0.993        0.993      1.000   \n",
      "bmi                0.968          0.969        0.969      0.989   \n",
      "\n",
      "               weight_pounds    bmi  \n",
      "height_cm              0.992  0.968  \n",
      "height_inches          0.993  0.969  \n",
      "height_feet            0.993  0.969  \n",
      "weight_kg              1.000  0.989  \n",
      "weight_pounds          1.000  0.989  \n",
      "bmi                    0.989  1.000  \n",
      "6\n",
      "\n",
      "Highly correlated pairs (>0.95):\n",
      "  height_cm ‚Üî height_inches: 1.000\n",
      "  height_cm ‚Üî height_feet: 1.000\n",
      "  height_cm ‚Üî weight_kg: 0.993\n",
      "  height_cm ‚Üî weight_pounds: 0.992\n",
      "  height_cm ‚Üî bmi: 0.968\n",
      "  height_inches ‚Üî height_feet: 1.000\n",
      "  height_inches ‚Üî weight_kg: 0.993\n",
      "  height_inches ‚Üî weight_pounds: 0.993\n",
      "  height_inches ‚Üî bmi: 0.969\n",
      "  height_feet ‚Üî weight_kg: 0.993\n",
      "  height_feet ‚Üî weight_pounds: 0.993\n",
      "  height_feet ‚Üî bmi: 0.969\n",
      "  weight_kg ‚Üî weight_pounds: 1.000\n",
      "  weight_kg ‚Üî bmi: 0.989\n",
      "  weight_pounds ‚Üî bmi: 0.989\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1. PHYSICAL MEASUREMENTS EXAMPLE:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Height in different units - highly correlated\n",
    "physical_data = pd.DataFrame({\n",
    "    'height_cm': [170, 175, 165, 180, 160, 185, 172, 168, 177, 162],\n",
    "    'height_inches': [66.9, 68.9, 65.0, 70.9, 63.0, 72.8, 67.7, 66.1, 69.7, 63.8],\n",
    "    'height_feet': [5.57, 5.74, 5.41, 5.91, 5.25, 6.07, 5.64, 5.51, 5.81, 5.31],\n",
    "    'weight_kg': [70, 80, 60, 90, 55, 95, 75, 65, 85, 58],\n",
    "    'weight_pounds': [154, 176, 132, 198, 121, 209, 165, 143, 187, 128],\n",
    "    'bmi': [24.2, 26.1, 22.0, 27.8, 21.5, 27.7, 25.3, 23.0, 27.1, 22.1]\n",
    "})\n",
    "\n",
    "print(\"Physical measurements dataset:\")\n",
    "print(physical_data.round(2))\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = physical_data.corr()\n",
    "print(f\"\\nCorrelation Matrix:\")\n",
    "print(corr_matrix.round(3))\n",
    "\n",
    "# Identify highly correlated pairs\n",
    "def find_high_correlations(df, threshold=0.9):\n",
    "    corr_matrix = df.corr().abs()\n",
    "    high_corr_pairs = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            correlation = corr_matrix.iloc[i, j]\n",
    "            if correlation > threshold:\n",
    "                feature1 = corr_matrix.columns[i]\n",
    "                feature2 = corr_matrix.columns[j]\n",
    "                high_corr_pairs.append((feature1, feature2, correlation))\n",
    "    \n",
    "    return high_corr_pairs\n",
    "\n",
    "high_corr = find_high_correlations(physical_data, 0.95)\n",
    "print(f\"\\nHighly correlated pairs (>0.95):\")\n",
    "for f1, f2, corr in high_corr:\n",
    "    print(f\"  {f1} ‚Üî {f2}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86fa3569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VARIANCE THRESHOLD DIMENSIONALITY REDUCTION EXAMPLES\n",
      "============================================================\n",
      "Remove features with low variance (little predictive information)\n"
     ]
    }
   ],
   "source": [
    "# Add this cell to demonstrate variance threshold dimensionality reduction\n",
    "\n",
    "print(\"VARIANCE THRESHOLD DIMENSIONALITY REDUCTION EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Remove features with low variance (little predictive information)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3641e837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. BASIC VARIANCE THRESHOLD EXAMPLE:\n",
      "========================================\n",
      "Sample data with different variance levels:\n",
      "   constant_feature  low_variance  medium_variance  high_variance  \\\n",
      "0                 1             1                1              1   \n",
      "1                 1             1                2             10   \n",
      "2                 1             1                3              5   \n",
      "3                 1             1                2             15   \n",
      "4                 1             2                4              8   \n",
      "5                 1             1                3             20   \n",
      "6                 1             1                2              3   \n",
      "7                 1             1                1             12   \n",
      "\n",
      "   useful_feature  \n",
      "0              10  \n",
      "1              20  \n",
      "2              30  \n",
      "3              40  \n",
      "4              50  \n",
      "5              60  \n",
      "6              70  \n",
      "7              80  \n",
      "\n",
      "Variance of each feature:\n",
      "  constant_feature: 0.000\n",
      "  low_variance: 0.125\n",
      "  medium_variance: 1.071\n",
      "  high_variance: 40.500\n",
      "  useful_feature: 600.000\n",
      "\n",
      "Applying Variance Threshold (threshold = 1.0):\n",
      "Original features: ['constant_feature', 'low_variance', 'medium_variance', 'high_variance', 'useful_feature']\n",
      "Selected features: ['high_variance', 'useful_feature']\n",
      "Removed features: ['constant_feature', 'low_variance', 'medium_variance']\n",
      "\n",
      "Before: 5 features\n",
      "After: 2 features\n",
      "Reduction: 3 features removed\n",
      "\n",
      "Filtered dataset:\n",
      "   high_variance  useful_feature\n",
      "0              1              10\n",
      "1             10              20\n",
      "2              5              30\n",
      "3             15              40\n",
      "4              8              50\n",
      "5             20              60\n",
      "6              3              70\n",
      "7             12              80\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1. BASIC VARIANCE THRESHOLD EXAMPLE:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Create sample data with different variance levels\n",
    "sample_data = pd.DataFrame({\n",
    "    'constant_feature': [1, 1, 1, 1, 1, 1, 1, 1],        # Zero variance\n",
    "    'low_variance': [1, 1, 1, 1, 2, 1, 1, 1],            # Very low variance\n",
    "    'medium_variance': [1, 2, 3, 2, 4, 3, 2, 1],         # Medium variance\n",
    "    'high_variance': [1, 10, 5, 15, 8, 20, 3, 12],       # High variance\n",
    "    'useful_feature': [10, 20, 30, 40, 50, 60, 70, 80]   # Good variance\n",
    "})\n",
    "\n",
    "print(\"Sample data with different variance levels:\")\n",
    "print(sample_data)\n",
    "\n",
    "# Calculate variance for each feature\n",
    "variances = sample_data.var()\n",
    "print(f\"\\nVariance of each feature:\")\n",
    "for col, var in variances.items():\n",
    "    print(f\"  {col}: {var:.3f}\")\n",
    "\n",
    "# Apply Variance Threshold\n",
    "print(f\"\\nApplying Variance Threshold (threshold = 1.0):\")\n",
    "\n",
    "selector = VarianceThreshold(threshold=1.0)\n",
    "selected_features = selector.fit_transform(sample_data)\n",
    "\n",
    "# Get feature names that were selected\n",
    "feature_names = sample_data.columns[selector.get_support()]\n",
    "result_df = pd.DataFrame(selected_features, columns=feature_names)\n",
    "\n",
    "print(f\"Original features: {list(sample_data.columns)}\")\n",
    "print(f\"Selected features: {list(feature_names)}\")\n",
    "print(f\"Removed features: {[col for col in sample_data.columns if col not in feature_names]}\")\n",
    "\n",
    "print(f\"\\nBefore: {sample_data.shape[1]} features\")\n",
    "print(f\"After: {result_df.shape[1]} features\")\n",
    "print(f\"Reduction: {sample_data.shape[1] - result_df.shape[1]} features removed\")\n",
    "\n",
    "print(f\"\\nFiltered dataset:\")\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993c7c58",
   "metadata": {},
   "source": [
    "WHAT IS EXHAUSTIVE FEATURE SELECTION?\n",
    "=============================================\n",
    "\n",
    "üîç EXHAUSTIVE SEARCH: Tests every possible combination of features\n",
    "üìä PERFORMANCE-BASED: Selects features that give best model performance  \n",
    "üéØ OPTIMAL SOLUTION: Guaranteed to find the best feature subset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
